{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SkovanskiyS/ailab/blob/main/Lab_2B_Perceptron_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27567c84",
      "metadata": {
        "id": "27567c84"
      },
      "source": [
        "# Lab 2B: Perceptron Tutorial\n",
        "\n",
        "## Objective\n",
        "This notebook is written as a **guided tutorial**.\n",
        "\n",
        "For each concept:\n",
        "1. We first **solve a problem together**.\n",
        "2. Then you are asked to **solve a similar problem yourself**.\n",
        "\n",
        "By the end, you will understand how perceptrons work and how they implement logical gates."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47ca920a",
      "metadata": {
        "id": "47ca920a"
      },
      "source": [
        "## Section 1: Perceptron Prediction (Worked Example)\n",
        "\n",
        "**Goal:** Understand how weights and bias produce an output.\n",
        "\n",
        "We start with a perceptron **without learning**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4542f29",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4542f29",
        "outputId": "672af950-c2b1-47d2-dd5a-a0ff363e64d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Worked example results:\n",
            "[2 3] 0\n",
            "[1 1] 1\n",
            "[3 1] 1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def step(z):\n",
        "    return 1 if z >= 0 else 0\n",
        "\n",
        "def perceptron(x, w, b):\n",
        "    return step(np.dot(x, w) + b)\n",
        "\n",
        "# Example inputs\n",
        "X = np.array([[2, 3], [1, 1], [3, 1]])\n",
        "\n",
        "# Chosen weights and bias\n",
        "w = np.array([1, -1])\n",
        "b = 0\n",
        "\n",
        "print(\"Worked example results:\")\n",
        "for x in X:\n",
        "    print(x, perceptron(x, w, b))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "594c9be0",
      "metadata": {
        "id": "594c9be0"
      },
      "source": [
        "### Explanation\n",
        "- We compute a **dot product** between inputs and weights\n",
        "- Add bias\n",
        "- Apply the step function\n",
        "\n",
        "This is exactly the equation:  \n",
        "$y = f(\\mathbf{w} \\cdot \\mathbf{x} + b)$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "774ff7f9",
      "metadata": {
        "id": "774ff7f9"
      },
      "source": [
        "### ✏️ Student Exercise 1\n",
        "Change `w` and `b` so that:\n",
        "- First input → output 1\n",
        "- Second input → output 0\n",
        "- Third input → output 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f67c9c29",
      "metadata": {
        "id": "f67c9c29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c92286d-1528-45a8-cc2d-bb6cb3f3adb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student Exercise 1 results:\n",
            "[2 3] 1\n",
            "[1 1] 0\n",
            "[3 1] 1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def step(z):\n",
        "    return 1 if z >= 0 else 0\n",
        "\n",
        "def perceptron(x, w, b):\n",
        "    return step(np.dot(x, w) + b)\n",
        "\n",
        "# Example inputs\n",
        "X = np.array([[2, 3], [1, 1], [3, 1]])\n",
        "\n",
        "# TODO: try different w and b\n",
        "w = np.array([1, 1])\n",
        "b = -3\n",
        "\n",
        "print(\"Student Exercise 1 results:\")\n",
        "for x in X:\n",
        "    print(x, perceptron(x, w, b))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf2cc6d6",
      "metadata": {
        "id": "cf2cc6d6"
      },
      "source": [
        "## Section 2: Training a Perceptron (Worked Example)\n",
        "\n",
        "**Goal:** See how learning updates weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b157aef7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b157aef7",
        "outputId": "05be3db5-6fd8-4d98-83f6-a45373785c44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: w=[0.2 0.1], b=0.0\n",
            "Epoch 1: w=[0.2 0.1], b=-0.1\n",
            "Epoch 2: w=[0.2 0.1], b=-0.20000000000000004\n",
            "Epoch 3: w=[0.1 0. ], b=-0.30000000000000004\n",
            "Epoch 4: w=[0.3 0.3], b=-0.30000000000000004\n"
          ]
        }
      ],
      "source": [
        "X = np.array([[2,3], [1,1], [2,1], [3,2]])\n",
        "y = np.array([1, 0, 0, 1])\n",
        "\n",
        "w = np.zeros(2)\n",
        "b = 0\n",
        "lr = 0.1\n",
        "\n",
        "for epoch in range(5):\n",
        "    for i in range(len(X)):\n",
        "        y_hat = perceptron(X[i], w, b)\n",
        "        error = y[i] - y_hat\n",
        "        w += lr * error * X[i]\n",
        "        b += lr * error\n",
        "    print(f\"Epoch {epoch}: w={w}, b={b}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64a8121a",
      "metadata": {
        "id": "64a8121a"
      },
      "source": [
        "### Explanation\n",
        "- If prediction is wrong, error ≠ 0\n",
        "- We adjust weights and bias\n",
        "- Over epochs, the model improves\n",
        "\n",
        "**This is learning.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18a86ed0",
      "metadata": {
        "id": "18a86ed0"
      },
      "source": [
        "### ✏️ Student Exercise 2\n",
        "Change the learning rate to `0.01` and `1.0`.\n",
        "Observe how convergence changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7f856eb6",
      "metadata": {
        "id": "7f856eb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af73b6e4-b29b-42f4-d70a-9174c17b9f7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training with learning rate = 0.01 ---\n",
            "Epoch 0: w=[0.02 0.01], b=0.0\n",
            "Epoch 1: w=[0.02 0.01], b=-0.01\n",
            "Epoch 2: w=[0.01 0.  ], b=-0.02\n",
            "Epoch 3: w=[0.03 0.03], b=-0.019999999999999997\n",
            "Epoch 4: w=[0.03 0.03], b=-0.03\n",
            "\n",
            "--- Training with learning rate = 1.0 ---\n",
            "Epoch 0: w=[2. 1.], b=0.0\n",
            "Epoch 1: w=[2. 1.], b=-1.0\n",
            "Epoch 2: w=[2. 1.], b=-2.0\n",
            "Epoch 3: w=[1. 0.], b=-3.0\n",
            "Epoch 4: w=[3. 3.], b=-3.0\n"
          ]
        }
      ],
      "source": [
        "# TODO: experiment with learning rate\n",
        "\n",
        "def step(z):\n",
        "    return 1 if z >= 0 else 0\n",
        "\n",
        "def perceptron(x, w, b):\n",
        "    return step(np.dot(x, w) + b)\n",
        "\n",
        "X_train = np.array([[2,3], [1,1], [2,1], [3,2]])\n",
        "y_train = np.array([1, 0, 0, 1])\n",
        "\n",
        "# Experiment with learning rate = 0.01\n",
        "w_01 = np.zeros(2)\n",
        "b_01 = 0\n",
        "lr_01 = 0.01\n",
        "\n",
        "print(\"\\n--- Training with learning rate = 0.01 ---\")\n",
        "for epoch in range(5):\n",
        "    for i in range(len(X_train)):\n",
        "        y_hat = perceptron(X_train[i], w_01, b_01)\n",
        "        error = y_train[i] - y_hat\n",
        "        w_01 += lr_01 * error * X_train[i]\n",
        "        b_01 += lr_01 * error\n",
        "    print(f\"Epoch {epoch}: w={w_01}, b={b_01}\")\n",
        "\n",
        "# Experiment with learning rate = 1.0\n",
        "w_1 = np.zeros(2)\n",
        "b_1 = 0\n",
        "lr_1 = 1.0\n",
        "\n",
        "print(\"\\n--- Training with learning rate = 1.0 ---\")\n",
        "for epoch in range(5):\n",
        "    for i in range(len(X_train)):\n",
        "        y_hat = perceptron(X_train[i], w_1, b_1)\n",
        "        error = y_train[i] - y_hat\n",
        "        w_1 += lr_1 * error * X_train[i]\n",
        "        b_1 += lr_1 * error\n",
        "    print(f\"Epoch {epoch}: w={w_1}, b={b_1}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2895dfc0",
      "metadata": {
        "id": "2895dfc0"
      },
      "source": [
        "## Section 3: Logical Gates with Perceptrons\n",
        "\n",
        "**Goal:** Understand perceptrons as logical decision units."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aee6c9ce",
      "metadata": {
        "id": "aee6c9ce"
      },
      "source": [
        "### Worked Example: AND Gate\n",
        "\n",
        "Truth table:\n",
        "\n",
        "| x₁ | x₂ | AND |\n",
        "|----|----|-----|\n",
        "| 0 | 0 | 0 |\n",
        "| 0 | 1 | 0 |\n",
        "| 1 | 0 | 0 |\n",
        "| 1 | 1 | 1 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5f17531",
      "metadata": {
        "id": "a5f17531"
      },
      "outputs": [],
      "source": [
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "\n",
        "# AND gate parameters\n",
        "w = np.array([1, 1])\n",
        "b = -1.5\n",
        "\n",
        "print(\"AND gate results:\")\n",
        "for x in X:\n",
        "    print(x, perceptron(x, w, b))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59b793dc",
      "metadata": {
        "id": "59b793dc"
      },
      "source": [
        "### Explanation\n",
        "- Only when both inputs are 1 does the sum exceed the threshold\n",
        "- AND is **linearly separable**, so one perceptron is enough"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2eaadaa",
      "metadata": {
        "id": "c2eaadaa"
      },
      "source": [
        "### ✏️ Student Exercise 3: OR Gate\n",
        "Implement the OR gate using a perceptron.\n",
        "\n",
        "Truth table:\n",
        "| x₁ | x₂ | OR |\n",
        "|----|----|----|\n",
        "| 0 | 0 | 0 |\n",
        "| 0 | 1 | 1 |\n",
        "| 1 | 0 | 1 |\n",
        "| 1 | 1 | 1 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "80ebe57c",
      "metadata": {
        "id": "80ebe57c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30c21e5c-fe83-4247-eeec-baf65bf948e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OR gate results:\n",
            "[0 0] 0\n",
            "[0 1] 1\n",
            "[1 0] 1\n",
            "[1 1] 1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def step(z):\n",
        "    return 1 if z >= 0 else 0\n",
        "\n",
        "def perceptron(x, w, b):\n",
        "    return step(np.dot(x, w) + b)\n",
        "\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "\n",
        "# TODO: choose w and b for OR gate\n",
        "w = np.array([1, 1])\n",
        "b = -0.5\n",
        "\n",
        "print(\"OR gate results:\")\n",
        "for x in X:\n",
        "    print(x, perceptron(x, w, b))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbcadb28",
      "metadata": {
        "id": "fbcadb28"
      },
      "source": [
        "## Section 4: XOR Gate – Why It Fails\n",
        "\n",
        "**Goal:** Discover the limitation of a single perceptron."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88867dcb",
      "metadata": {
        "id": "88867dcb"
      },
      "outputs": [],
      "source": [
        "y_xor = np.array([0,1,1,0])\n",
        "\n",
        "print(\"Try to solve XOR with one perceptron:\")\n",
        "w = np.array([1, 1])\n",
        "b = -1\n",
        "\n",
        "for x in X:\n",
        "    print(x, perceptron(x, w, b))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e98c05f8"
      },
      "source": [
        "# Task\n",
        "Execute the code cell `88867dcb` to demonstrate the failure of a single perceptron to correctly produce the XOR truth table outputs, then explain why a single perceptron cannot solve the XOR problem, focusing on the concept of linear separability and how the perceptron's decision boundary works, and finally, summarize the key takeaway from the XOR gate section regarding the limitations of a single perceptron."
      ],
      "id": "e98c05f8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44b825a3"
      },
      "source": [
        "## Execute XOR Failure Example\n",
        "\n",
        "### Subtask:\n",
        "Execute the code cell `88867dcb` to demonstrate that a single perceptron with given weights and bias cannot correctly produce the XOR truth table outputs.\n"
      ],
      "id": "44b825a3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b7f1130"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires executing the code cell `88867dcb` to show the failure of a single perceptron for XOR. This step will execute the provided code.\n",
        "\n"
      ],
      "id": "2b7f1130"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22d61597",
        "outputId": "71ab3689-a070-470b-c76e-eefac7fa5d47"
      },
      "source": [
        "y_xor = np.array([0,1,1,0])\n",
        "\n",
        "print(\"Try to solve XOR with one perceptron:\")\n",
        "w = np.array([1, 1])\n",
        "b = -1\n",
        "\n",
        "for x in X:\n",
        "    print(x, perceptron(x, w, b))"
      ],
      "id": "22d61597",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Try to solve XOR with one perceptron:\n",
            "[0 0] 0\n",
            "[0 1] 1\n",
            "[1 0] 1\n",
            "[1 1] 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4673273"
      },
      "source": [
        "### Why XOR Fails with a Single Perceptron\n",
        "\n",
        "The output from the previous cell clearly shows that a single perceptron, even with adjusted weights and bias, cannot perfectly reproduce the XOR truth table. Specifically, for the input `[1, 1]`, the perceptron outputs `1`, while the XOR gate should output `0`.\n",
        "\n",
        "**Linear Separability:**\n",
        "The fundamental reason for this failure lies in the concept of **linear separability**. A single perceptron works by drawing a single straight line (or a hyperplane in higher dimensions) to separate the different classes of data points. Data points on one side of the line are classified as one output (e.g., 0), and points on the other side are classified as the other output (e.g., 1).\n",
        "\n",
        "Let's consider the XOR inputs and desired outputs:\n",
        "- `[0, 0]` -> `0`\n",
        "- `[0, 1]` -> `1`\n",
        "- `[1, 0]` -> `1`\n",
        "- `[1, 1]` -> `0`\n",
        "\n",
        "If we plot these points on a 2D graph:\n",
        "- `(0, 0)` is a `0`\n",
        "- `(0, 1)` is a `1`\n",
        "- `(1, 0)` is a `1`\n",
        "- `(1, 1)` is a `0`\n",
        "\n",
        "You'll notice that there is no single straight line that can separate the `0` outputs (`[0, 0]` and `[1, 1]`) from the `1` outputs (`[0, 1]` and `[1, 0]`). The `0` points are diagonally opposite, and the `1` points are also diagonally opposite, making it impossible to draw a single linear boundary to correctly classify all points.\n",
        "\n",
        "**Key Takeaway:**\n",
        "The XOR problem demonstrates a critical limitation of a single-layer perceptron: it can only learn **linearly separable** patterns. For problems that are not linearly separable, such as XOR, a single perceptron is insufficient. To solve non-linearly separable problems, a more complex architecture, such as a multi-layer perceptron (with hidden layers), is required. These multi-layer networks can learn non-linear decision boundaries by combining multiple linear boundaries."
      ],
      "id": "e4673273"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fdb74de"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "*   A single perceptron cannot solve the XOR problem because the XOR function is not linearly separable. This means that its outputs (0 for `[0,0]` and `[1,1]`, 1 for `[0,1]` and `[1,0]`) cannot be perfectly separated by a single straight line (or hyperplane) in a 2D plot.\n",
        "*   The key takeaway from the XOR gate section is that a single-layer perceptron is fundamentally limited to solving problems that are linearly separable. For problems with non-linearly separable patterns, such as the XOR problem, a more complex architecture like a multi-layer perceptron (with hidden layers) is required.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   When attempting to solve the XOR problem with a single perceptron using weights `w = [1, 1]` and bias `b = -1`, the perceptron produced the following outputs for the given inputs:\n",
        "    *   `[0, 0]` -> `0`\n",
        "    *   `[0, 1]` -> `1`\n",
        "    *   `[1, 0]` -> `1`\n",
        "    *   `[1, 1]` -> `1`\n",
        "*   Comparing these results to the actual XOR truth table outputs (`[0, 1, 1, 0]`), the perceptron incorrectly classified the input `[1, 1]`, outputting `1` instead of the correct XOR output of `0`.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The inability of a single perceptron to correctly classify the XOR inputs highlights its limitation to only learn linearly separable patterns.\n",
        "*   To address non-linearly separable problems like XOR, multi-layer perceptrons, which can form non-linear decision boundaries by combining multiple linear boundaries, are necessary.\n"
      ],
      "id": "7fdb74de"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}